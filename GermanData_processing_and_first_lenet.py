# -*- coding: utf-8 -*-
"""Processing_and_Lenet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rxBjR2cf_uWRToIbcjBETGsscxAPjmuy
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# ! pip install kaggle
# ! mkdir ~/.kaggle
# ! cp kaggle.json ~/.kaggle/
# ! chmod 600 ~/.kaggle/kaggle.json
# ! kaggle datasets download meowmeowmeowmeowmeow/gtsrb-german-traffic-sign
# ! unzip gtsrb-german-traffic-sign.zip
# 
# ! kaggle datasets download andrewmvd/road-sign-detection
# ! unzip road-sign-detection.zip

import pandas as pd
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as T
import torch
import torch.nn as nn
from torchvision.utils import make_grid
from torchvision.utils import save_image
from IPython.display import Image
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
from matplotlib import image
import pickle

meta = pd.read_csv('Meta.csv')
train = pd.read_csv('Train.csv')
test = pd.read_csv('Test.csv')

# look for 0-3 Speed Limit, 14 - Stop, 27 - Crosswalk
# select class id: 0, 1, 2, 3, 14, 27
# modify class labels 
classID = [0, 1, 2, 3, 14, 27]
train = train.loc[train['ClassId'].isin(classID)]
print('Train Size: ' + str(train.shape))
test = test.loc[test['ClassId'].isin(classID)]
print('Test Size: ' + str(test.shape))

# 1 - speed limit, 2 - stopsign, 3 cross walk

train['ClassId'] = train['ClassId'].replace({0:0, 1:0, 2:0, 3:0, 14:1, 27:2})
test['ClassId'] = test['ClassId'].replace({0:0, 1:0, 2:0, 3:0, 14:1, 27:2})

# load image data and prepare for preprocessing
train_path = train['Path']
train_imgs = []
for path in train_path:
  im = Image.open(path)
  # width, height = im.size   # Get dimensions
  # left = (width - 400)/2
  # top = (height - 400)/2
  # right = (width + 400)/2
  # bottom = (height + 400)/2
  # # Crop the center of the image
  # im = im.crop((left, top, right, bottom))
  im = im.resize((48, 48))
  train_imgs.append(np.asarray(im, dtype=int))

train_imgs[0]

train_imgs = np.array(train_imgs, dtype=int)
train_y = np.array(train['ClassId'])

# load image data and prepare for preprocessing
test_path = test['Path']
test_imgs = []
for path in test_path:
  im = Image.open(path)
  # width, height = im.size   # Get dimensions
  # left = (width - 400)/2
  # top = (height - 400)/2
  # right = (width + 400)/2
  # bottom = (height + 400)/2
  # # Crop the center of the image
  # im = im.crop((left, top, right, bottom))
  im = im.resize((48, 48))
  test_imgs.append(np.asarray(im, dtype=int))

test_imgs = np.array(test_imgs)
test_y = np.array(test['ClassId'])

plt.imshow(test_imgs[20])

train_y

X_train = train_imgs 
y_train = train_y

X_test = test_imgs
y_test = test_y

from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.14, random_state=42)

print(f"Length of Train Data : {len(X_train)}")
print(f"Length of Validation Data : {len(X_valid)}")

# Number of training examples
n_train = X_train.shape[0]

# Number of testing examples.
n_test = X_test.shape[0]

# What's the shape of an traffic sign image?
image_shape = X_train.shape[1:]

# How many unique classes/labels there are in the dataset.
n_classes = len(np.unique(y_train))

print("Number of training examples =", n_train)
print("Number of testing examples =", n_test)
print("Image data shape =", image_shape)
print("Number of classes =", n_classes)
print('Done')

truelabels = np.genfromtxt('signnames.csv', skip_header=1, dtype=[('myint','i8'), ('mysring','S55')], delimiter=',')

truelabels

import numpy as np
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
import random
import csv

# we will use this function to plot many images further in the project
def plot_figures(figures, nrows = 1, ncols=1, labels=None):
    fig, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=(12, 14))
    axs = axs.ravel()
    for index, title in zip(range(len(figures)), figures):
        axs[index].imshow(figures[title], plt.gray())
        if(labels != None):
           axs[index].set_title(labels[index])
        else:
            axs[index].set_title(title)
            
        axs[index].set_axis_off()
        
    plt.tight_layout()

number_to_stop = 8
figures = {}
labels = {}
for i in range(number_to_stop):
    index = random.randint(0, n_train-1)
    labels[i] = truelabels[y_train[index]][1].decode('ascii')
    #print(labels)
    figures[i] = X_train[index]
    
plot_figures(figures, 2, 4, labels)
print('Done')

### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include 
### converting to grayscale, etc.
### Feel free to use as many code cells as needed.
from sklearn.utils import shuffle
X_train_rgb = X_train
X_train_gray = np.sum(X_train/3, axis=3, keepdims=True)

X_test_rgb = X_test
X_test_gray = np.sum(X_test/3, axis=3, keepdims=True)

X_valid_rgb = X_valid
X_valid_gray = np.sum(X_valid/3, axis=3, keepdims=True)

print(X_train_rgb.shape)
print(X_train_gray.shape)

print(X_test_rgb.shape)
print(X_test_gray.shape)

X_train = X_train_gray
X_test = X_test_gray
X_valid = X_valid_gray
X_train, y_train = shuffle(X_train, y_train)

image_depth_channels = X_train.shape[3]

number_to_stop = 8
figures = {}
random_signs = []
for i in range(number_to_stop):
    index = random.randint(0, n_train-1)
    labels[i] = truelabels[y_train[index]][1].decode('ascii')
    figures[i] = X_train[index].squeeze()
    random_signs.append(index)
    
#print(random_signs)
plot_figures(figures, 2, 4, labels)
print('Done')



#creating copies of the training data
X_train_temp= X_train
y_train_temp= y_train

y_train

import cv2
from PIL import Image

X_train= X_train_temp
y_train= y_train_temp 
unique_train, counts_train = np.unique(y_train, return_counts=True)



def augment_dataset(X_train,y_train):
    transformed_X_train = []
    transformed_y_train = []
    new_counts_train = counts_train
    count=1
    print('No. of classes: '+str(len(new_counts_train)))
    for i in range(n_train):
        count+=1
        if(new_counts_train[y_train[i]] < 3000):
            times= (3000-new_counts_train[y_train[i]])//100
            for j in range(times):
                dimension1= X_train[i].shape[0] # gives 48 in this scenario
                dimension2= X_train[i].shape[1] # gives 48 in this scenario

                ####################### Transform #############################
                dx, dy = np.random.uniform(-3.0, 3.0, 2)    
                M = np.float32([[1, 0, dx], [0, 1, dy]])
                dst = cv2.warpAffine(X_train[i], M, (dimension1, dimension2)) ##shifts image up,down, right,left on the image pane
                dst = dst[:,:,None]

                ######################### Perspective change #############################

                upper_bound = random.randint(25, dimension2) #from 25 to 30
                lower_bound = random.randint(0, 5) # from 0 to 5
                points_one = np.float32([[0,0],[dimension1,0],[0,dimension2],[dimension1,dimension2]])
                points_two = np.float32([[0, 0], [upper_bound, lower_bound], [lower_bound, dimension2],[dimension1, upper_bound]])
                M = cv2.getPerspectiveTransform(points_one, points_two)
                dst = cv2.warpPerspective(dst, M, (dimension1,dimension2))

                ####################### ROTATION ###############################

                tilt = random.randint(-15, 15)
                M = cv2.getRotationMatrix2D((dimension1/2, dimension2/2), tilt, 1)
                dst = cv2.warpAffine(dst, M, (X_train[i].shape[0], X_train[i].shape[1]))
                transformed_X_train.append(dst)
                transformed_y_train.append(y_train[i])

                new_counts_train[y_train[i]] += 1

    transformed_X_train= np.array(transformed_X_train)
    transformed_X_train = np.reshape(transformed_X_train, (np.shape(transformed_X_train)[0], 48, 48, 1))

    return np.concatenate((X_train, transformed_X_train), axis=0),np.concatenate((y_train, transformed_y_train), axis=0)
     
X_train,y_train= augment_dataset(X_train,y_train)
print('total no. of images in the modified dataset: '+ str(len(X_train)))
print('Done')

print(counts_train)

unique_train, counts_train = np.unique(y_train, return_counts=True)
plt.bar(unique_train, counts_train)
plt.grid()
plt.title("Train Dataset Sign Counts")
plt.show()

unique_test, counts_test = np.unique(y_test, return_counts=True)
plt.bar(unique_test, counts_test)
plt.grid()
plt.title("Test Dataset Sign Counts")
plt.show()

unique_valid, counts_valid = np.unique(y_valid, return_counts=True)
plt.bar(unique_valid, counts_valid)
plt.grid()
plt.title("Valid Dataset Sign Counts")
plt.show()

"""Normalization"""

def normalize(x_label):
    return (x_label-127.5)/127.5

X_train_normalized=  normalize(X_train)
X_test_normalized=  normalize(X_test)
X_valid_normalized= normalize(X_valid)

number_to_stop = 8
figures = {}
count = 0
for i in random_signs:
    labels[count] = truelabels[y_train[i]][1].decode('ascii')
    figures[count] = X_train_normalized[i].squeeze()
    count += 1;
    
plot_figures(figures, 2, 4, labels)

X_train = X_train_normalized
X_test = X_test_normalized
X_valid= X_valid_normalized

#!pip install tensorflow

import tensorflow as tf

train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
test_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))

BATCH_SIZE = 64
SHUFFLE_BUFFER_SIZE = 100

train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
test_dataset = test_dataset.batch(BATCH_SIZE)

train_dataset

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(6, (5, 5), activation = 'relu', input_shape = (48, 48, 1), name = '1st_conv'),
    tf.keras.layers.AveragePooling2D(2, 2, name = '1st_pooling'),
    tf.keras.layers.Conv2D(16, (5, 5), activation = 'relu', name = '2nd_conv'),
    tf.keras.layers.AveragePooling2D(2, 2, name = '2nd_pooling'),
    tf.keras.layers.Flatten(name = 'flatten'),
    tf.keras.layers.Dropout(0.5, name = '1st_dropout'),
    tf.keras.layers.Dense(120,activation='relu', name = '1st_fc'),
    tf.keras.layers.Dense(67,activation='relu', name = '2nd_fc'),
    tf.keras.layers.Dense(3, activation = 'softmax', name = '3rd_fc')
])

model.summary()

model.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)

callbacks = tf.keras.callbacks.EarlyStopping(
    monitor = 'val_sparse_categorical_accuracy', 
    patience = 10, 
    mode = 'auto'
)

history = model.fit(
    train_dataset,
    epochs=100,
    callbacks = [callbacks],
    validation_data=test_dataset,
)

# list all data in history
print(history.history.keys())

# summarize history for accuracy
plt.plot(history.history['sparse_categorical_accuracy'])
plt.plot(history.history['val_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Save Model
model.save("LeNet_trained")

from tensorflow import keras
model = keras.models.load_model('./LeNet_trained')

y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score
print(f"LeNet model on tested data has accuracy of {accuracy_score(y_test, np.argmax(y_pred,axis=1))}")

y_pred.shape

y_test.shape

y_pred[0]

y_pred = y_pred.argmax(axis=-1)

import matplotlib.pyplot as plt 
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import roc_curve, auc, roc_auc_score


target= [0,1,2]

# set plot figure size
fig, c_ax = plt.subplots(1,1, figsize = (12, 8))

# function for scoring roc auc score for multi-class
def multiclass_roc_auc_score(y_test, y_pred, average="macro"):
    lb = LabelBinarizer()
    lb.fit(y_test)
    y_test = lb.transform(y_test)
    y_pred = lb.transform(y_pred)

    for (idx, c_label) in enumerate(target):
        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])
        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))
    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')
    return roc_auc_score(y_test, y_pred, average=average)


print('ROC AUC score:', multiclass_roc_auc_score(y_test, y_pred))

c_ax.legend()
c_ax.set_xlabel('False Positive Rate')
c_ax.set_ylabel('True Positive Rate')
plt.show()

